## QEUR22_EXAMPLE01:　事例(1)～ 手書き数字の判別(python)

## ～　ブログの効用とPythonエコシステム　～

### ・・・　（能書きは）前回のつづきです　・・・

QEU:FOUNDER （設定65歳） ： “これをJulia基礎編の最後のシリーズにしましょう。ここでは事例として「手書き数字の判別」をやりましょう。いまでは手書き数字の判別は機械学習の基礎編だが、1990年後半にアメリカの郵便局が大々的に、この技術を応用したんだよね。さて・・・。前回の〇〇学部の件に戻るんだが・・・。まさか学部が足らないことが、Ｊ国の弱点だと思っているの！？”

![imageJL1-75-1](https://introJL1973.github.io/images/imageJL1-75-1.jpg)

D先生 （設定65歳）： “お偉い方は、単純に「そう」思っているんじゃないかネェ・・・。ちなみに、この番組は**「高齢者によるイノベーション」**です。若者よ。安心しなさい・・・。我々、高齢者がついている・・・。”

![imageJL1-75-2](https://introJL1973.github.io/images/imageJL1-75-2.jpg)

QEU:FOUNDER ： “持論だが・・・。J国の機械学習がなぜ遅れているのか・・・。その、最も大きな原因は「J国の学生がブログを書かない」ことにあります。I国の天才のノート(↑)にあるように、ブログを書くことは自らの学習成果を定着させるのに最も重要なプロセスです。でも、J語の機械学習関連の記事はほとんどがプロの書いたモノです。これではいけない。A国はそこそこ・・・、C国はもっとスゴイよ！！！”

D先生 ： “C国の機械学習ブログ群の強烈さは圧倒されます。その一方、C国のブログはPythonがらみばかりで、Juliaがすくない。そこに寧ろC国のJulia熱の不気味さを感じるわけで・・・。”

![imageJL1-75-3](https://introJL1973.github.io/images/imageJL1-75-3.jpg)

QEU:FOUNDER ： “今は動画の影響でブログを書くことは「金（広告）にならなく」なってしまいました。でも、ブログは少なくとも学生には重要な効用があります。学生の機械学習の能力を評価したいのであれば、「どこどこの学校を卒業しました」って卒業証書は必要ありません。”

D先生： “「わかっている人」が学生の書いたブログを読めば、一発でその人のレベルがわかりますね。ブログは立派な履歴書になります。”

QEU:FOUNDER  ： “・・・というわけで、今回は小生が収集した（一流）ブログ群を活用して、どう機械学習プログラムを開発するかのデモをやってみましょう。まずは、このブログから始めましょう。”

![imageJL1-75-4](https://introJL1973.github.io/images/imageJL1-75-4.jpg)

QEU:FOUNDER  ： “そして、プログラムをドン！！”

```python
# -------------
# Principal Component Analysis
# -------------
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
from sklearn.decomposition import PCA

# ---
# loading digit data:
# ---
from sklearn.datasets import load_digits
digits = load_digits()
digits.data.shape
#(1797, 64)

# ---
# ロードしたデータの内容を表示する
# ---
def plot_digits(data):
    fig, axes = plt.subplots(4, 10, figsize=(10, 4),
                             subplot_kw={'xticks':[], 'yticks':[]},
                             gridspec_kw=dict(hspace=0.1, wspace=0.1))
    for i, ax in enumerate(axes.flat):
        ax.imshow(data[i].reshape(8, 8),
                  cmap='binary', interpolation='nearest',
                  clim=(0, 16))
plot_digits(digits.data)
print(digits.target[:40])

```

![imageJL1-75-5](https://introJL1973.github.io/images/imageJL1-75-5.jpg)

D先生： “ほう・・・。簡単な方（8x8）の「手書き数字データセット」を使いましたか・・・。Kerasには、もっとすごいモノ(28x28)があるはずですが・・・。”

![imageJL1-75-6](https://introJL1973.github.io/images/imageJL1-75-6.jpg)

QEU:FOUNDER  ： “「あの」、**Yann LeCun**じゃないですか・・・。F社の役員だったとは知らなかったわ・・・。このような話もブログを調査すると、ついでに得られるわけで・・・。”

```python
# ---
# PCAメトリックスの表現力を目視で確認する
# ---
pca = PCA(2)  # project from 64 to 2 dimensions
projected = pca.fit_transform(digits.data)
print(digits.data.shape)
print(projected.shape)

plt.scatter(projected[:, 0], projected[:, 1],
    c=digits.target, edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('nipy_spectral', 10))
plt.xlabel('PC_1')
plt.ylabel('PC_2')
plt.colorbar();

```

![imageJL1-75-7](https://introJL1973.github.io/images/imageJL1-75-7.jpg)

```python
# ---
# PCAメトリックスの累積寄与率変化を表示する
# ---
pca = PCA().fit(digits.data)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');

```

![imageJL1-75-8](https://introJL1973.github.io/images/imageJL1-75-8.jpg)

D先生： “この手のPCAのお作法については、とても多くの文献やブログに載っているので、コピーライトもあったもんじゃないですね。知らなかった人は、このグラフ(↑)を見て、PCAの威力に対して素直に感動してください（笑）。”

![imageJL1-75-9](https://introJL1973.github.io/images/imageJL1-75-9.jpg)

QEU:FOUNDER  ： “じゃあ、いよいよディープラーニングの学習プロセスにいきます。次に参考にするブログをコレ（↑）に変えました。”

```python
# ---
# ディープラーニングに移行しましょう
# 学習データ(train-test)を準備する
# ---
from sklearn.model_selection import train_test_split

# ----
# PCAを使う
pca = PCA(0.9)
X   = pca.fit_transform(digits.data)
print(pca.n_components_)    # 21次元

# Datasetを分割する
y   = digits.target
x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=11)
print(x_test.shape)
print(y_test)

# ---
# KERASによる学習
# ----
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import np_utils
from keras.optimizers import RMSprop

y_train = np_utils.to_categorical(y_train)
y_test_org = y_test.copy()
y_test = np_utils.to_categorical(y_test)
print(y_test)

batch_size = 64
num_classes = 10
epochs = 20

model = Sequential()
model.add(Dense(256, activation='relu', input_shape=(21,)))
model.add(Dense(256, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
model.summary()

# ---
# DL実行する
model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(), metrics=['accuracy'])

history = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,
                    validation_data=(x_test, y_test))

```

**（学習パラメタ）**

![imageJL1-75-10](https://introJL1973.github.io/images/imageJL1-75-10.jpg)

**（学習過程）**

![imageJL1-75-11](https://introJL1973.github.io/images/imageJL1-75-11.jpg)

D先生： “このブログではCIFER10(32x32x3)データセットで学習していたんでしょ？ずいぶん学習対象を軽くしたものですね。”

![imageJL1-75-12](https://introJL1973.github.io/images/imageJL1-75-12.jpg)

QEU:FOUNDER  ： “自分の安いPCでやろうとおもったら、CIFERを計算するのはキツイよ。MINISTだったら、20-EPOCH程度は、**あっという間に学習できます**。”

![imageJL1-75-13](https://introJL1973.github.io/images/imageJL1-75-13.jpg)

QEU:FOUNDER  ： “最後の判別パフォーマンス評価の部分も良い先生をみつけました。ただし、少し改造しないと動きませんからね・・・。”

```python
# ---
# 学習ヒストリーをプロットする
def plot_history(history):
    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]
    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]
    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]
    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]
    
    if len(loss_list) == 0:
        print('Loss is missing in history')
        return 
    
    ## As loss always exists
    epochs = range(1,len(history.history[loss_list[0]]) + 1)
    
    ## Loss
    plt.figure(1)
    for l in loss_list:
        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))
    for l in val_loss_list:
        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))
    
    plt.title('Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    
    ## Accuracy
    plt.figure(2)
    for l in acc_list:
        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')
    for l in val_acc_list:    
        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')

    plt.title('Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

plot_history(history)

```
**（パフォーマンス１）**

![imageJL1-75-14](https://introJL1973.github.io/images/imageJL1-75-14.jpg)

**（パフォーマンス2）**

![imageJL1-75-15](https://introJL1973.github.io/images/imageJL1-75-15.jpg)

```python
# ---
# CFマトリックスを作成する
import itertools
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# クラス分類
classes = [0,1,2,3,4,5,6,7,8,9]

def plot_confusion_matrix(cm, classes, normalize=False, cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        title='Normalized confusion matrix'
    else:
        title='Confusion matrix'

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    #plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()
    
## multiclass or binary report
## If binary (sigmoid output), set binary parameter to True
def full_multiclass_report(model, x, y_true, classes):
    
    # Predict classes and stores in y_pred
    pb_pred = model.predict(x)
    y_pred  = []
    for i in range(pb_pred.shape[0]):
        y_idx = np.argmax(pb_pred[i,:])
        y_pred.append(y_idx)
    #print("y_true:",y_true)
    #print("y_pred:",y_pred)
    
    # Print accuracy score
    print("Accuracy : "+ str(accuracy_score(y_true,y_pred)))
    print("")
    
    # Print classification report
    print("Classification Report")
    print(classification_report(y_true,y_pred,digits=5))    
    
    # Plot confusion matrix
    cnf_matrix = confusion_matrix(y_true,y_pred)
    print(cnf_matrix)
    plot_confusion_matrix(cnf_matrix,classes=classes)
# ----
full_multiclass_report(model, x_test, y_test_org, classes)

```

**（評価メトリックス群）**

![imageJL1-75-16](https://introJL1973.github.io/images/imageJL1-75-16.jpg)

**（混合マトリックス）**

![imageJL1-75-17](https://introJL1973.github.io/images/imageJL1-75-17.jpg)

D先生 ： “これは、FOUNDERにはとても出来そうもない美しいプログラムになりました・・・（笑）。どれぐらいの作業負荷になりました。”

QEU:FOUNDER ： “**Webサーチへの負荷量は8/10で、プログラム改造が2/10になる**かな？ブログの文章を読むのに時間をかけています。実際には、ブログを読まずともプログラムを直接見れば使えるかがわかります。”

D先生 ： “（読むのは）ムダじゃないですか？”

QEU:FOUNDER ： “読むことによって、**「エコシステム」に対する理解**がさらに深まります。これは、次への投資ですよ。学生は他の学生が書いたブログを読み、自分がブログを書くことでエコシステムを育てることになります。以上のことを総括すると、J国の機械学習のレベルは・・・。”

D先生： “全く、お話にならない・・・。A国、C国に対して・・・。”

QEU:FOUNDER  ： “これは技術的なものでも、投資的なものでもありません。民族文化の問題じゃないかな？”


## ～　まとめ　～

### ・・・　前回のつづきです　・・・

QEU:FOUNDER ： “昔、東南アジアの旧植民地で**「プランテーション」**ってあったじゃない？現地住民の生活水準が上がらすに、かわいそうだなぁという・・・。でも、たとえ工業化されても、製品が「安いモノ」であれば同じことなんですよ。自由貿易であれば、強者は他国を戦争で支配する必要もない。自分が高いモノを作り、他の国が安いモノを作ればいいだけです。”

![imageJL1-75-18](https://introJL1973.github.io/images/imageJL1-75-18.jpg)

C部長 : “J国は30年間も、**全速力で最悪の方向に走った**。”

QEU:FOUNDER ： “J国はモノを安くすることばかりを考えた・・・。そして、上は下に対して押し付けた（最適化）。でも、Ｊ国に近い位置にあるＴＷ（国）の動向を見て、皆がそれぞれ模索をしているのだと思いました。あの選挙の結果、個人的に大いに驚きました。”

![imageJL1-75-19](https://introJL1973.github.io/images/imageJL1-75-19.jpg)

QEU:FOUNDER ： “ただね、小生が間違っていただけなんだろうけどね。この動画の後半の台湾についてのコメントを見てください。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/bTY8l0Pipdg" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

C部長 : “**C国と仲良くしたいという民意が強い**ようです。これもおどろき・・・(笑)。”

QEU:FOUNDER ： “そうそう・・・。笑っちゃうくらい驚き・・・（笑）。我々は、たぶんかなり偏ったメディアに支配されているんですよ。さて、TWといえば半導体なんだけと・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/cUn6fjfwhao" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

C部長 : “最近、安全保障の観点からA国がとりこもうとしているが・・・。”

QEU:FOUNDER ： “C国は今でも半導体のナンバーワンのわけです。ここで質問だか・・・。”

![imageJL1-75-20](https://introJL1973.github.io/images/imageJL1-75-20.jpg)

QEU:FOUNDER ： “**TWはC国とケンカをしたいと思う？**”

C部長 : “それは思わないでしょう・・・。”

QEU:FOUNDER ： “さらに質問です。A国は、どんどん半導体産業の自国回帰を推進しています。この状況を、TWは本心は歓迎していると？”

C部長 : “それは（歓迎）していないでしょう。当面はいいことを言われていると思いますが、あとでどうなるか・・・。”

QEU:FOUNDER ： “いままでの議論から鑑みるに、宮〇先生の説明（↓）のように、**あの選挙は彼らが自主的に判断したごく自然な結論だと思うんです。**よく考えてみると、(TWが)独立する(independent)のに、（A国に）依存する(dependent)のはおかしいと思わない？”

<iframe width="560" height="315" src="https://www.youtube.com/embed/qqFINRVY0Xg" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

QEU:FOUNDER ： “独立って言葉は21世紀ではすでに幼稚なんですよ。独立は自主の要件の一部でしかない。その点、彼らは〇ズじゃないわけで・・・（爆）。”
