## QEUR22_DAPL04:　応用編(4)～異常検出 - exoplanetを語る

## ～　「均質」という前提が大切・・・　～

D先生 （設定65歳）： “前回で前処理が終了しました。残るは異常検出です・・・。ちなみに、この番組は「高齢者によるイノベーション」です。オレたちはすごいだゾ。加〇臭は伊達じゃない。漏れたちの力で、J国は復活だ！”

**（ポジティブ）**

![imageJL1-72-1](https://introJL1973.github.io/images/imageJL1-72-1.jpg)

**（ネガティブ）**

![imageJL1-72-2](https://introJL1973.github.io/images/imageJL1-72-2.jpg)

QEU:FOUNDER （設定65歳） ： “「波形」が機械学習データセットになっているケースが多くないので、今回はexoplanetのデータセットをプロセスの異常検出のために使用しています。もちろん、今回のデータの性質は意図と違うので判別精度が上がらないことは承知のうえでね。”

D先生 ： “判別手法はマハラノビス距離を使うんですか？もし、それで判別するのであれば学習データはNEGATIVEだけになるはずです。Train-Positiveの多量のデータは使わないんですか？”

QEU:FOUNDER ： “もちろん、パフォーマンス評価時に使います・・・。マハラノビス距離については説明する必要もないよね。それでは、プログラムをドン！！CSVデータを4種類読み込んで処理しているので、プログラムは長いが、実はいたってシンプルです。”

```python
#=
#   PYTHONで新しい、プロセス異常検出ロジックをやってみる
#   引用：Exoplanet_Huntingの機械学習
#   MAHALANOBIS距離（均質な空間が一つだけが前提）
#=
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import math
from scipy.spatial import distance

# -----
# CSV読み込みとlog変換関数
def log_readCSV(name_csvin):

    # -----
    # << train_positives >>
    df_x = pd.read_csv(name_csvin)
    del df_x["p19"]
    del df_x["m19"]
    df_x = df_x.loc[:,"p1":"m10"]
    df_x.head()
    # -----
    # マトリックスを生成する
    mx_x = df_x.values + 1
    for i in range(mx_x.shape[0]):
        for j in range(mx_x.shape[1]):
            mx_x[i,j] = round(math.log10(mx_x[i,j]),4)
    #print(mx_x)
    # -----
    # マトリックスの項目平均を生成する
    arr_xmean = []
    for j in range(mx_x.shape[1]):
        arr_xmean.append(round(np.mean(mx_x[:,j]),4))

    return np.array(arr_xmean), mx_x

# -----
# << train_positives >>
arr_train_positives, mx_train_positives = log_readCSV('csv_train_positives.csv')
print("--- arr_train_positives ---")
print(arr_train_positives)
# -----
# << train_negatives >>
arr_train_negatives, mx_train_negatives = log_readCSV('csv_train_negatives.csv')
# -----
# << test_positives >>
arr_test_positives, mx_test_positives   = log_readCSV('csv_test_positives.csv')
# -----
# << test_negatives >>
arr_test_negatives, mx_test_negatives   = log_readCSV('csv_test_negatives.csv')
print("--- mx_test_negatives ---")
print(mx_test_negatives)

# -----
dim_x_comm  = 20
dim_y_train_positives = mx_train_positives.shape[0]
dim_y_train_negatives = mx_train_negatives.shape[0]
dim_y_test_positives  = mx_test_positives.shape[0]
dim_y_test_negatives  = mx_test_negatives.shape[0]
print("dim_x_comm: ", dim_x_comm)
print("dim_y_train_positives: ", dim_y_train_positives)
print("dim_y_train_negatives: ", dim_y_train_negatives)
print("dim_y_test_positives:  ", dim_y_test_positives)
print("dim_y_test_negatives:  ", dim_y_test_negatives)

# ------
# 分散共分散行列を計算する
# ベース・マトリックスはtrain-negative(exoでない)
cov = np.cov(mx_train_negatives.T)
# 分散共分散行列の逆行列を計算
cov_i = np.linalg.pinv(cov)
# 2つの標本 ベクトルA と ベクトルB のマハラノビス距離を計算する
# ベクトルA ：　test-positiveのメンバ（i番目）
# ベクトルB : train-positiveの平均値
#d = distance.mahalanobis(vector_A, vector_B, cov_i)

# ------
# マハラノビス距離を計算(1)
# A:計測・マトリックスはtest-positive
# B:ベース・マトリックスはtrain-negative
vector_B = arr_train_negatives
arrd_test_positive = []
for i in range(dim_y_test_positives):
    vector_A = mx_test_positives[i]
    d = distance.mahalanobis(vector_A, vector_B, cov_i)
    arrd_test_positive.append(round(d,4))
    print("データ{0}番目, マハラノビス距離の計算結果: {1}".format(i,d))
# ---
print("--- arrd_test_positive ---")
print(arrd_test_positive)

# ------
# マハラノビス距離を計算(2)
# A:計測・マトリックスはtrain-positive
# B:ベース・マトリックスはtrain-negative
arrd_train_positive = []
for i in range(dim_y_train_positives):
    vector_A = mx_train_positives[i]
    d = distance.mahalanobis(vector_A, vector_B, cov_i)
    arrd_train_positive.append(round(d,4))
    #print("データ{0}番目, マハラノビス距離の計算結果: {1}".format(i,d))
# ---
print("--- arrd_train_positive ---")
print(arrd_train_positive)

# ------
# マハラノビス距離を計算(3)
# A:計測・マトリックスはtest-negative
# B:ベース・マトリックスはtrain-negative
arrd_test_negative = []
for i in range(dim_y_test_negatives):
    vector_A = mx_test_negatives[i]
    d = distance.mahalanobis(vector_A, vector_B, cov_i)
    arrd_test_negative.append(round(d,4))
    #print("データ{0}番目, マハラノビス距離の計算結果: {1}".format(i,d))
# ---
print("--- arrd_test_negative ---")
print(arrd_test_negative)
# -----
# POSITIVEは学習に使っていないので、データをまとめる
arrd_all_positive = arrd_train_positive + arrd_test_positive
print(arrd_all_positive)

# -----
# 二重ヒストグラムを描く
import numpy as np
import matplotlib.pyplot as plt
# グラフ設定
myfig = plt.figure(figsize=(10,6))
# ヒストグラム
plt.title("MAHARANOBIS DISTANCE")
bins = np.linspace(0.0, 15, 30)
plt.hist(arrd_test_negative, bins, alpha = 0.5, label='negative')
plt.hist(arrd_all_positive, bins, alpha = 0.5, label='positive')
plt.xlabel("distance")
plt.ylabel("frequency")
plt.legend(loc='best')
plt.show()

```

![imageJL1-72-3](https://introJL1973.github.io/images/imageJL1-72-3.jpg)

```python
# -----
# パフォーマンス表を作成する
val_threshold = 5   # しきい値
# ----
# その１(NEGATIVE)
sum_negative_NO = 0     # exoでないのにexoでないと判定（正解）
sum_negative_YES = 0     # exoでないのにexoであると判定（誤り）
for i in range(dim_y_test_negatives):
    if arrd_test_negative[i] > val_threshold:
        sum_negative_YES += 1
    else:
        sum_negative_NO += 1
print("sum_negative_NO: ",sum_negative_NO)
print("sum_negative_YES: ",sum_negative_YES)

# ----
# その2(POSITIVE)
sum_positive_NO = 0     # exoであるのにexoでないと判定（誤り）
sum_positive_YES = 0     # exoであるのにexoであると判定（正解）
for i in range(dim_y_train_positives+dim_y_test_positives):
    if arrd_all_positive[i] > val_threshold:
        sum_positive_YES += 1
    else:
        sum_positive_NO += 1
print("sum_positive_NO: ",sum_positive_NO)
print("sum_positive_YES: ",sum_positive_YES)

# ----
# 正確度の計算
val_acc = (sum_negative_NO+sum_positive_YES)/(dim_y_test_negatives+dim_y_train_positives+dim_y_test_positives)
print("判定正確度-val_acc: ",val_acc)

```

![imageJL1-72-4](https://introJL1973.github.io/images/imageJL1-72-4.jpg)

D先生 ： “やっぱりパフォーマンスはダメダメでした・・・。”

QEU:FOUNDER ： “個人的には思ったよりも高いのでビックリしたがね・・・（笑）。なぜパフォーマンスが低いんだろうか・・・。D先生、思考題です・・・。”

![imageJL1-72-5](https://introJL1973.github.io/images/imageJL1-72-5.jpg)

D先生 ： “そうですね。**マハラノビス距離が予測として意味をもつには、学習データ（単位空間）が均質である必要があります。**今回の場合、いろんなモノが混じっているんじゃないですか？”

QEU:FOUNDER ： “マハラノビス法とかタグチメソッドの改造版であるRT法とかは、1つの学習データ群に一つのメトリックスになりますからね。外観検査のように、正解（標準データ）が決まっている場合には簡単に適応するのだけど、プロセスの場合には複数の標準状態があるかもしれないし・・・。”

D先生 ： “プロセス管理の場合、無理にマハラノビス法を使わず、他の方法を使えばいいじゃないですか・・・。”

QEU:FOUNDER ： “率直、そう思うね・・・。次はJulia言語になるのだが、マハラノビス距離を使うのがいやになってきたねえ・・・。”

D先生 ： “そうですね・・・。**Random Forestあたり**がいいんじゃないですか？”

QEU:FOUNDER ： “小生は隠れディープラーニング嫌いなので、先生の意見に賛成します（笑）。手法は後で決めましょう。”


## ～　まとめ　～


C部長 : “最近、FOUNDERは、最近「ノリ」が悪くなりました。**イケメンバトル**のころはよかったのに・・・。”

![imageJL1-72-6](https://introJL1973.github.io/images/imageJL1-72-6.jpg)

QEU:FOUNDER ： “そうだったっけ・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/O-yRmrabQbc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

C部長 : “この先生(↑)は好きですか？”

QEU:FOUNDER ： “そりゃあもう大好きです。とても個性的だからね・・・。名前とルックスのギャップもステキです（笑）。”

C部長 : “(彼は)保守ですよね。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/HWOkG7Z0y5I" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

QEU:FOUNDER ： “保守？いや・・・、**世間的には彼ら（↑）こそ「保守」でしょう**。カッコつき・・・(笑)。番組終了記念としてブログに貼らせていただきました・・・。”

C部長 : “えっ！？（あの番組を）見たんですか！？”

QEU:FOUNDER ： “小生が見るわきゃあない・・・。イワシの頭みたいに飾っておいたの・・・。鬼が逃げるかと思って・・・（笑）。”

