## QEUR22_LINK03:　旧ROUND2-1リンク解説（ロボティックス編）

## ～　「がっかり・・・」と「無限の可能性」が同居した不思議な感覚・・・　～

QEU:FOUNDER （設定65歳） ： “さて、旧ROUND解説シリーズも最後になりました。**「QEUロボティックス編」**について簡単に紹介をしてみましょう。この番組は「高齢者によるイノベーション」です。”

**（モジュールの仕上がり）**
- 解析技術編　→　20％
- 自動検査技術編　→　70％
- ロボティックス編　→　40％

D先生 （設定65歳）： “現時点のロボティックス編は、**「理論の枠組みもまともにできていない状況」**ですよね。”

![imageJL1-32-1](https://introJL1973.github.io/images/imageJL1-32-1.jpg)

QEU:FOUNDER  ： “まあ、そうですがね。しかし、ある意味「達観」すれば、まあ40％ぐらいは仕上がっていると見ることができます。機械学習における強化学習を知らない人ならば、このQEUロボティックスを始めるには（強化学習の）代表的な例題をやっておいた方がいいです。細かい理論は、別に知っておく必要はないから・・・。”

D先生 （設定65歳）： “代表的な例題って、OpenAIが提供している環境のこと？Mountain Carとか、Cart Poleとか・・・。”

QEU:FOUNDER ： “ああ・・・。その程度でいい・・・。そして、できればQテーブルとDQN（ディープラーニング）でやってみてほしい。昔、初めてやってみたときこ、初めは**「すごい！こんなことができるのか！」**と感動し、後で**「あ～あ、この程度しかできないのか」**とがっかりした（笑）。”

![imageJL1-32-2](https://introJL1973.github.io/images/imageJL1-32-2.jpg)

D先生 ： “あれ？いきなり、天下のDeepMxxdさんをディスる発言・・・。”

![imageJL1-32-3](https://introJL1973.github.io/images/imageJL1-32-3.jpg)


QEU:FOUNDER  ： “どんなに複雑なゲームでも、それが**Single-Agent**である限り、コンピュータは必ず人間に勝てますよ。でも、Multi-Agentになるとコンピュータは人間にまるっきし勝てません。すごいのは、人間はあれほど多様で複雑なMulti-Agentタスクを人間の一生（例えば60年）というごく短い時間に習得しているんです。すごいと思わない？”

D先生 ： “人間は強化学習でも転移学習ができるんでしょう。「ハイハイ、ヨチヨチ・・・」というマルチタスクを学習している赤ちゃんは天才・・・。一方、それが成長して微分・積分ごときでウンウン苦労している人間は、時間と共にどんどん〇カになっている。”

QEU:FOUNDER ： “・・・（爆）。さて、旧ROUNDで我々は強化学習で使える2種の新しいメトリックスを提案しました。”

- **畳み込みRT法**
- **（符号付き）minRT法**

QEU:FOUNDER ： “これらの解説の代わりに、以下、ブログの解説を始めましょう。まずは、コレ（↓）から・・・。”

<div class="blogcardfu" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;"><a href="https://jpnqeur21rl2048.blogspot.com/2022/05/qeur21rl2048t102048.html" target="_blank" style="display:block;text-decoration:none;"><span class="blogcardfu-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;"><img src="https://capture.heartrails.com/100x100?https://jpnqeur21rl2048.blogspot.com/2022/05/qeur21rl2048t102048.html" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;"></span><br style="display:none"><span class="blogcardfu-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">QEUR21_RL2048T10:　ゲーム2048強化学習の準備をする</span><br><span class="blogcardfu-content" style="font-size:87.5%;font-weight:400;color:#666666;">            ～　ワタシ、あこがれていたの・・・　～     D先生 ： “外観検査機のミニ・プロジェクトが理想的な形でおわりました。我々は新RTメトリックスという新しい武器を得たわけです。”         （今回のパフォーマンス：新RT法）         （参考：...</span><br><span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span></a></div>

D先生： “あっ・・・。そうでしたっけ・・・・。外観検査機のためにRTメトリックスの改良をした後で始めたプロジェクトでしたね。”

QEU:FOUNDER  ： “旧ラウンドで使ったのは「スライドRT法」と無理やり名付けた特徴量です。まあ、率直、**「畳み込みRT法」**のほうがいいよね。次回、もう一度やるのであれば畳み込みRT法でやると思います。パターンを分析するゲームとしては、もうひとつコレ（↓）もあるよね・・・。”

<div class="blogcardfu" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;"><a href="https://jpnqeur21rlmsp.blogspot.com/2022/07/qeur21rldqn1rtdqn-er1.html" target="_blank" style="display:block;text-decoration:none;"><span class="blogcardfu-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;"><img src="https://capture.heartrails.com/100x100?https://jpnqeur21rlmsp.blogspot.com/2022/07/qeur21rldqn1rtdqn-er1.html" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;"></span><br style="display:none"><span class="blogcardfu-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">QEUR21_RLDQN1:　本実験～畳み込みRT法プラスDQN-ERでマインスイーパー(その1)</span><br><span class="blogcardfu-content" style="font-size:87.5%;font-weight:400;color:#666666;">            ～　「ここまで来たか」という感じ・・・　～     D先生 ： “今回から本実験に移行します。新RTメトリックスと DQN with Experience Replay 法を使って強化学習をします。今回から、やっとε-Greedyの出番ですね。イプシロンの...</span><br><span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span></a></div>

D先生 ： “このプロジェクトでは、すでに畳み込みRT法を使っています。”

QEU:FOUNDER ： “なぜ畳み込みRT法が重要なのか・・・。現状で画像パターンを扱う強化学習手法はCNN（ディープラーニング）法です。CNNなのでPCスペックが高くなるし、学習コストも高くなります。それを回避するために畳み込みRTメトリックスを提案しているんです。このプロジェクトをもう一度やるのだったら、Julia言語で計算が速くなるのでより長い時間（ステップ）を計算して、どこまでいけるのかをやってみたいね。”

D先生 ： “畳み込みRTメトリックスはCNNの劣化版、ある意味・・・。どの程度の**「そこそこ性能」**になるのかは、とても興味があります。”

QEU:FOUNDER ： “ゲーム2048であれば畳み込みRTメトリックスはCNNとかなり**「よい勝負」**になると思います。でも、マインスイーパーではちょっときついかな・・・。さて、畳み込みRTメトリックスの応用事例はここまで・・・。それでは、次にminRTメトリックスの応用にいきましょう。まずはコレ（↓）・・・。崖歩きだ！ドン！！”

<div class="blogcardfu" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;"><a href="https://jpnqeur21rlclf.blogspot.com/2022/07/qeur21rlclf0-cliffwalking.html" target="_blank" style="display:block;text-decoration:none;"><span class="blogcardfu-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;"><img src="https://capture.heartrails.com/100x100?https://jpnqeur21rlclf.blogspot.com/2022/07/qeur21rlclf0-cliffwalking.html" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;"></span><br style="display:none"><span class="blogcardfu-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">QEUR21_RLCLF0 – 「崖歩き(Cliff_Walking)」をさらっと復習しましょう(その１)</span><br><span class="blogcardfu-content" style="font-size:87.5%;font-weight:400;color:#666666;">            ～　一回休みの「初めの一歩」　～     QEU:FOUNDER ; “さあて、今シリーズでは息抜きとして「崖（cliff）」について話そうと思います。コレ（↓）のこと・・・。”         D先生 : “なんだ…、 「Cliff-Walking」とい...</span><br><span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span></a></div>

D先生 ： “このminRTメトリックスには**「符号」**を付けないんですよね・・・。”

QEU:FOUNDER ： “環境の非対称性が強すぎるので符号をつけられません。それでも、このメトリックスを加えるだけで**学習収束は圧倒的にパワーアップ**します。まあ、簡単な環境なので準備運動としてやっておきたいですね。”

D先生 ： “minRT関連では、本番はコレ（↓）、**「2D(二次元)メイズ」**になるんですか？”

<div class="blogcardfu" style="width:auto;max-width:9999px;border:1px solid #E0E0E0;border-radius:3px;margin:10px 0;padding:15px;line-height:1.4;text-align:left;background:#FFFFFF;"><a href="https://jpnqeur21of2dmz.blogspot.com/2022/07/qeur212dmz5dqn-with-experience-replay.html" target="_blank" style="display:block;text-decoration:none;"><span class="blogcardfu-image" style="float:right;width:100px;padding:0 0 0 10px;margin:0 0 5px 5px;"><img src="https://capture.heartrails.com/100x100?https://jpnqeur21of2dmz.blogspot.com/2022/07/qeur212dmz5dqn-with-experience-replay.html" width="100" style="width:100%;height:auto;max-height:100px;min-width:0;border:0 none;margin:0;"></span><br style="display:none"><span class="blogcardfu-title" style="font-size:112.5%;font-weight:700;color:#333333;margin:0 0 5px 0;">QEUR21_2DMZ5:　フィードバックによる学習加速（DQN with experience replay）</span><br><span class="blogcardfu-content" style="font-size:87.5%;font-weight:400;color:#666666;">            ～　これが、本プロジェクトの成果「その一」　 ～     ・・・　前回のつづきです　・・・        QEU:FOUNDER : “ライムラインRTはゲームの評価に使いましょう。つぎは、いよいよフィードバック制御です。タイムラインminRTメトリックス...</span><br><span style="clear:both;display:block;overflow:hidden;height:0;">&nbsp;</span></a></div>

D先生 ： “ああ、メイズ（迷路）ですね。そうか・・・、そういえばQEUロボティックスの偉大なイノベーションには**「フィードバック学習」**があったんだ・・・。偉大って、誇張しすぎですか？”

QEU:FOUNDER ： “偉大（な発明）だと思いますよ（笑）。ただし、フィードバック学習は適切なメトリックスがないと実現できません。あと、このRTメトリックスは符号をつけると効果的です。うまく言えば、「座標変換」が可能です。”

## XY座標　→　RTメトリックス

D先生 ： “このプロジェクトは「上積み」あるのかねぇ・・・。FOUNDER、このプロジェクトはJulia言語でやってみるだけですよね・・・。”

QEU:FOUNDER ： “いや・・・、**「あること」**を考えている。”

D先生 ： “えっ？そうなの？何ですか？”

QEU:FOUNDER ： “それは秘密・・・（笑）。いまのところ、本当にできるかどうかはわからないんで・・・。”

D先生： “ほう・・・。ヒントだけでも・・・。”

QEU:FOUNDER  ： “Qテーブルでもない、ディープラーニングでもない強化学習・・・。”

D先生 ： “は？興味がありますね。あと残っているのは、「３Dメイズ」ですよね。これについては紹介しないんですか？”

QEU:FOUNDER ： “（紹介は）いらないでしょう。2Dメイズをさらに進化させて、やっと3Dメイズが**「まな板に載る」**感じです。さらにいえば、Julia言語でプログラムを組まないと、いくら時間があっても足りません。”

D先生 ： “これで旧ラウンド(ROUND2-1)のLOOK-BACKは終わりです。ちょっと2つ質問させてください。このQEUロボティックスが完成したとして、役に立つロボットができるとはあまりイメージできないです。”

QEU:FOUNDER ： “我々のターゲットはロボットというよりも、**「知能治具」**だよね。我々がやっていることはSingle-Agentなのでそんなに大したことができません。さらには、環境(Environment)に特化したメトリックスを使うので汎用性はないんですよ。”

![imageJL1-32-4](https://introJL1973.github.io/images/imageJL1-32-4.jpg)

QEU:FOUNDER ： “**はたして近年の「労働者のスキルが落ちている」のかは知りません（笑）**。しかし、仮に本当にスキルが落ちたとして、それをカバーするような治具が開発されていればいいじゃないですか・・・。”

D先生 ： “まあ、治具でカバーできればいいですよね。あと、もう一つ質問です。**SOART法やminRT法はタグチメソッドなんですか？**”

QEU:FOUNDER ： “**違います**。タグチメソッドのRT(Recognition-Taguchi)法から出発しましたが、**機械学習に合うように改良しているうちに別物になってしまいました**。”

D先生 ： “RTという言葉は過去の名残と・・・。**「アメリカ・インディアン」**みたいですね（笑）。”

QEU:FOUNDER ： “・・・（笑）。さて、ルックバックを終えて、地道にJulia言語の紹介をつづけましょう。Gradient Decent法が重要になるから、まずはそれをやろうか・・・。あっ、そうだ。重要なことをいいます。是非、カンパをください・・・。”

### [＞寄付のお願い(click here)＜}(https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

D先生 ： “どうぞ、よろしく。”


