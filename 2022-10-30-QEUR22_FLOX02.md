## QEUR22_FLUX02:　Gradient_Descent – Juliaの場合

## ～　そんなに簡単なワケがないじゃん・・・　～

### ・・・　前回のつづきです　・・・

D先生 （設定65歳）： “人生の大先輩の皆さま、イノベーションをやってますか？この番組は**「高齢者によるイノベーション」**です。現状のディープラーニングを使った強化学習（↓）は単純な環境でも本当に時間がかかります。でも、こんなに時間がかかるのはなぜ？そんなにインプット情報が必要なの？それともディープラーニングのせい？””

![imageJL1-61-1](https://introJL1973.github.io/images/imageJL1-61-1.jpg)

QEU:FOUNDER （設定65歳） ： “小生としては、「ここらへん」に興味があるわけです。近年の計算機パワーにモノを言わせた**「力任せ」の強化学習ロジックの進歩**はすごいです。でも、小生としては発展のベクトルをシンプル化に向けてもよいと思うんです。”

![imageJL1-61-2](https://introJL1973.github.io/images/imageJL1-61-2.jpg)

QEU:FOUNDER ： “プロセスが複雑な関数になる環境(ENVIRONMENT)があるとします。これは、そのままではディープラーニングでしか解けません。でも、**テクノメトリックスを使えば関数が大幅に簡単**になります。その結果、ディープラーニングのような手間がかかる手法を使わなくて済むというケースはあると思います。”

![imageJL1-61-3](https://introJL1973.github.io/images/imageJL1-61-3.jpg)

D先生 ： “なるほど、例えばε-Greedy法で最適行動選択をするとして、行動価値関数Qは精密に予測されている必要はないです。最適行動aの予測が正確であればいいわけで・・・。”

![imageJL1-61-4](https://introJL1973.github.io/images/imageJL1-61-4.jpg)

QEU:FOUNDER ： “・・・というわけで、「能書き」はおわり・・・（笑）。今回はGradient_Descent（GD：勾配降下法）をJulia言語で解きます。例によって、Julia でGDを解いた事例があったので、これを簡単化します。”

![imageJL1-61-5](https://introJL1973.github.io/images/imageJL1-61-5.jpg)

D先生 ： “前回のPython版と基本的な考え方は同じでしょ？PythonプログラムをJulia用に翻訳すればいいわけですよねぇ？”

QEU:FOUNDER ： “これも、ほとんど同じロジックです。でも、上記の資料のプログラムは1次回帰式における**「切片を使う、使わない」というオプション**もあり、ひょっとしたら今後、これを改造して使うかもしれないので紹介します。それでもコードをドン！！”

```julia
# Julia - Gradient Descent
using DataFrames, CSV
using Plots
# ----
# Read the file using CSV.File and convert it to DataFrame
df = DataFrame(CSV.File("Advertising.csv"))
#,TV,radio,newspaper,sales
first(df,5) #displaying the first 5 rows to get an overview of the dataset
describe(select!(df, Not(:Column1)))
# ----
y = df[!, :sales]; #Y-values
X = select(df, Not(:sales)); #features
target = convert(Array, y)
design_matrix = convert(Matrix, X)

# ----
# compute cost function helps us to compute the mean squared error
function compute_cost(X, y, theta)
    m = size(X)[1] # number of samples
    preds = X * theta #calculate predictions using theta
    loss = preds - y #calculate error
    # Half mean squared loss
    cost = (1/(2m)) * (loss' * loss)
    return cost
end

# ----
# Gradient Descent function to update the theta values
function gradient_descent(X, y, alpha, fit_intercept=true, n_iter=2000)
    m = length(y) # number of training examples 
    if fit_intercept
        # Add a bias
        b = ones(m, 1)
        X = hcat(b, X)
    else
        X
    end
     
    # Initializing theta
    theta = zeros(size(X)[2]) 
    # Initialise the cost vector
    cost = zeros(n_iter)
     
    #looping over the number of iterations
    for iter in range(1, stop=n_iter)
        pred = X * theta #predictions
        # Calculate the cost for each iter
        cost[iter] = compute_cost(X, y, theta)
        # Update the theta θ at each iter
        theta = theta - ((alpha/m) * X') * (pred - y);
    end
    return (theta, cost)
end

theta, cost = gradient_descent(design_matrix, target, 0.00001, true, 500)
theta

# ---
# plot the cost during training
plot(cost, label="Cost per iter", ylabel="Cost", xlabel="Number of Iteration",
title="Cost Per Iteration")

```

D先生 ： “使用するデータセットは前回のプログラムと同じです。”

![imageJL1-61-6](https://introJL1973.github.io/images/imageJL1-61-6.jpg)

QEU:FOUNDER ： “いちおう500回繰り返してみました。”

![imageJL1-61-7](https://introJL1973.github.io/images/imageJL1-61-7.jpg)

QEU:FOUNDER ： “グラフなんかどうでもいいですよね。ついでに計算時間を測定しました。”

![imageJL1-61-8](https://introJL1973.github.io/images/imageJL1-61-8.jpg)

D先生 ： “Python版プログラムではどうだったのですか？”

![imageJL1-61-9](https://introJL1973.github.io/images/imageJL1-61-9.jpg)

QEU:FOUNDER ： “Julia言語の方が若干速いよね。もちろん、ロジックが違うので単純な比較はできないけど・・・。”

D先生 ： “後で、この方法を使えるかもしれないって・・・？”

QEU:FOUNDER ： “このプログラムは多次元一次回帰式を解いています。もし、ちょっと改造して**「多次元二次回帰式」**を解ければどうします？”

D先生 ： “それは便利ですね・・・（笑）。”

QEU:FOUNDER ： “現実問題、ほとんどの単純なプロセスは**多次元2次方程式レベル**で解けますよ。**テクノメトリックスを適切に使えば**ね・・・。”

D先生 ： “「切片の要否」が必要なのは、単位空間の適用をにらんでいますね。”

QEU:FOUNDER ： “そういうこと・・・。”



## ～　まとめ　～

QEU:FOUNDER ： “Y先生が、またまた過激なことを・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/rG6LaIQEWLk" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

C部長 : “まったくケシカラン・・・。本質を言ってはいけないですよね。”

QEU:FOUNDER ： “これとは話は変わりますが、「お偉い方」がお小遣い欲しさに、ある組織に入り込み、その組織を腐らせるケースが多いですよね。”

C部長 : “その場合、組織の技術が発展しないケースが多いですよね。”

QEU:FOUNDER ： “そりやぁ・・・、自分がいる間だけ稼げればいいから・・・。技術を敢えて停滞させ、「イノベーション」とか「奥義」とかいう抽象的な言葉で飾って付加価値を上げるんです。もし技術が発展すると、過去の技術の価値が下がるので都合が悪いんです。”
